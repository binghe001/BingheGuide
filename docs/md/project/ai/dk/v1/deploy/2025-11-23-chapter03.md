---
title: 第03节：DeepSeek R1蒸馏模型组Ollama调用
pay: https://articles.zsxq.com/id_r3pjpefip1an.html
---

# 《实战AI大模型》部署大模型-第03节：DeepSeek R1蒸馏模型组Ollama调用

作者：冰河
<br/>星球：[http://m6z.cn/6aeFbs](http://m6z.cn/6aeFbs)
<br/>博客：[https://binghe.gitcode.host](https://binghe.gitcode.host)
<br/>文章汇总：[https://binghe.gitcode.host/md/all/all.html](https://binghe.gitcode.host/md/all/all.html)
<br/>源码获取地址：[https://t.zsxq.com/0dhvFs5oR](https://t.zsxq.com/0dhvFs5oR)

**大家好，我是冰河~~**

目前DeepSeek R1及其蒸馏模型都支持使用ollama进行调用，今天带着大家一起实现通过Ollama调用DeepSeek R1大模型。

## 一、Ollama调用DeepSeek说明

大家可以在Ollama支持的调用模型主页查看目前支持的模型情况，链接为：[https://ollama.com/library/deepseek-r1](https://ollama.com/library/deepseek-r1)

<div align="center">
    <img src="https://binghe.gitcode.host/images/project/ai/2026-11-23-001.png?raw=true" width="70%">
    <br/>
</div>

点击 View all可以查看所有的模型。

<div align="center">
    <img src="https://binghe.gitcode.host/images/project/ai/2026-11-23-002.png?raw=true" width="70%">
    <br/>
</div>

## 二、演示Ollama调用DeepSeek

这里，我们通过DeepSeek-r1:14b模型进行演示，在命令行终端输入如下命令安装并启动Ollama。

```bash
curl -fsSL https://ollama.com/install.sh | sh
# 安装完成后即可使用ollama start来启动ollama服务
ollama start
```

实际我们在《[第02节：DeepSeek R1蒸馏模型组本地部署与调用](https://articles.zsxq.com/id_kfxh0fvs4ig6.html)》一节中，我们通过Cloud Studio搭建的环境中，已经安装并启动了Ollama。所以，我们可以直接输入如下命令通过Ollama运行deepseek-r1:14b模型。

```bash
ollama run deepseek-r1:14b
```

运行命令的效果如下所示。

<div align="center">
    <img src="https://binghe.gitcode.host/images/project/ai/2026-11-23-003.png?raw=true" width="70%">
    <br/>
</div>

在命令行中输入你好，效果如下所示。

## 查看完整文章

加入[冰河技术](https://public.zsxq.com/groups/48848484411888.html)知识星球，解锁完整技术文章、小册、视频与完整代码

