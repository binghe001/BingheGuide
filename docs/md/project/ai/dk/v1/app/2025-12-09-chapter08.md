---
title: 第08节：基于Dify+deepseekR1搭建智能客服机器人
pay: https://articles.zsxq.com/id_tq4ws8po1arp.html
---

# 《实战AI大模型》生成AI应用-第08节：基于Dify+deepseekR1搭建智能客服机器人

作者：冰河
<br/>星球：[http://m6z.cn/6aeFbs](http://m6z.cn/6aeFbs)
<br/>博客：[https://binghe.gitcode.host](https://binghe.gitcode.host)
<br/>文章汇总：[https://binghe.gitcode.host/md/all/all.html](https://binghe.gitcode.host/md/all/all.html)
<br/>源码获取地址：[https://t.zsxq.com/0dhvFs5oR](https://t.zsxq.com/0dhvFs5oR)

**大家好，我是冰河~~**

Dify是一个开源的大语言模型（LLM）应用开发平台，旨在简化和加速生成式AI应用的创建和部署。该平台结合了后端即服务（Backend as Service, BaaS）和LLMOps的理念，为开发者提供了一个用户友好的界面和一系列强大的工具，使他们能够快速搭建生产级的AI应用。今天，我们就基于Dify+deepseekR1搭建智能客服机器人。

## 一、配置环境

### 1.1 环境信息

今天，我们部署DeepSeek的环境如下所示。

版本信息如下：

* 操作系统：centos 7.9

* docker版本：20.10.5-3

### 1.2 安装Docker环境

采用阿里云镜像源加速Docker安装过程，并进行性能优化配置：

```bash
yum install -y yum-utils device-mapper-persistent-data lvm2
yum install yum-utils -y
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates
yum install docker-ce-cli-20.10.5-3.el7 docker-ce-20.10.5-3.el7 -y
```

**Docker引擎优化配置：**

```bash
mkdir -p /etc/docker
cat > /etc/docker/daemon.json << EOF
{
    "max-concurrent-downloads": 10,
    "log-driver": "json-file",
    "log-level": "warn",
    "log-opts": {
        "max-size": "100m",
        "max-file": "3"
    },
    "live-restore": true,
    "exec-opts": ["native.cgroupdriver=systemd"],
    "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn"],
    "storage-driver": "overlay2",
    "storage-opts": [
        "overlay2.override_kernel_check=true"
    ]
}
EOF
```

**关键配置参数解析：**

- `max-concurrent-downloads`：提升镜像拉取速度
- `live-restore`：确保容器在Docker守护进程重启时不中断
- `registry-mirrors`：配置国内镜像源加速下载
- `storage-driver`：使用overlay2文件系统提高性能

**启动并验证Docker服务：**

```bash
systemctl daemon-reload
systemctl enable docker
systemctl restart docker
docker info | grep "Server Version"
```

## 二、部署Ollama

### 2.1 安装Ollama

选择Ollama作为模型服务框架，因其轻量化和易用性：

```bash
docker run --rm -v /tmp:/mnt swr.cn-south-1.myhuaweicloud.com/migrator/ollama-bin:0.5.7 cp /ollama /mnt
mv /tmp/ollama /usr/bin/
ollama -v
```

### 2.2 将ollama纳入systemd管理

将Ollama配置为systemd服务，确保服务稳定性和自动恢复

```bash
cat > /etc/systemd/system/ollama.service << EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
Environment="HOME=/root"
ExecStart=/usr/bin/ollama serve
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
EOF
```

### 2.3 配置自启动

启动ollama并开启自启动：

```bash
systemctl enable ollama
systemctl start ollama
systemctl status ollama
```

效果如下所示。

## 查看完整文章

加入[冰河技术](https://public.zsxq.com/groups/48848484411888.html)知识星球，解锁完整技术文章、小册、视频与完整代码